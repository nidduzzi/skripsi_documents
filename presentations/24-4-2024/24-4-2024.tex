\documentclass[a4paper,12pt]{scrreprt}
\usepackage[pdftex,bookmarks=true,unicode=true,pdfusetitle,bookmarksnumbered=true,bookmarksopen=true,breaklinks=true,pdfborder={0 0 1},backref=true,colorlinks=false]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}   % biar bisa kombinasi font
\usepackage{mathptmx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{dsfont}
\usepackage{listings}
\usepackage{relsize}
\usepackage{physics}
\usepackage{optidef}  % for writing optimization problems easier
\usepackage{algpseudocode}
\usepackage{algorithm}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\usepackage{booktabs}
\usepackage{cleveref}
\begin{document}
\chapter{Derivation}
To derive the least squares support vector regression (J.A. Suykens LSSVM Book) model we start with the linear expression:

\begin{equation}
        y=W^{\intercal}\vb{x}+b
\end{equation}

\begin{equation}
        \min_{W,e} J(W,e) = \frac{1}{2}W^{\intercal}W + C\frac{1}{2}\sum_{k=1}^{n}e_{k}
\end{equation}

Such that
\begin{equation}
        y_{k}=W^{\intercal}\vb{x}_{k}+b+e_{k} \qquad k=1,\dots,n
\end{equation}

\begin{equation}
        L(W,b,e;\vb{\alpha}) = \frac{1}{2}W^{\intercal}W + C\frac{1}{2}\sum_{k=1}^{n}e_{k} + \sum_{k=1}^{n}\alpha_{k} \left(W^{\intercal}\vb{x}_{k}+b+e_{k} - y_{k} \right)
\end{equation}

Derive the KKT system
\begin{align}
        \pdv{L}{W} = 0 \rightarrow          & W = \sum_{k=1}^{n}\alpha_{k}x_{k}                                \\
        \pdv{L}{b} = 0 \rightarrow          & \sum_{k=1}^{n}\alpha_{k} = 0                                     \\
        \pdv{L}{e_{k}} = 0 \rightarrow      & \alpha_{k} = C e_{k}                        & \qquad k=1,\dots,n \\
        \pdv{L}{\alpha_{k}} = 0 \rightarrow & W^{\intercal}\vb{x}_{k}+b+e_{k} - y_{k} = 0 & \qquad k=1,\dots,n \\
\end{align}

After eliminating $W$ and $\vb*{e}$, with $\vb{1}_{n} = \langle 1,\dots,1\rangle$, $\vb{y} = \lbrack y_1, \dots, y_n \rbrack $, and ${\alpha} = \lbrack \alpha_1, \dots, \alpha_n \rbrack $ the solution is as follows in block matrix notation
\begin{equation} \label{eq:lssvrSolution}
        \begin{bmatrix}
                0          & \vb{1}_{n}^{\intercal} \\
                \vb{1}_{n} & \Omega + \frac{I}{C}
        \end{bmatrix}
        \begin{bmatrix}
                b           \\
                \vb{\alpha} \\
        \end{bmatrix}
        =
        \begin{bmatrix}
                0      \\
                \vb{y} \\
        \end{bmatrix}
\end{equation}

\chapter{Psuedocode}
\section{Training}
The solution in \cref{eq:lssvrSolution} is rewritten in psuedocode in \cref{alg1:cap}

\begin{algorithm}[H]
        \caption{LSSVR Training}\label{alg1:cap}
        \begin{algorithmic}[1]
                \Require $\vb{X}, \vb{y}, \gamma$
                \Ensure $\vb{\alpha}, b$
                \State $\Omega \gets [[]]$ \Comment{Construct matrix of inner products in high dimensional space}

                \For{$k=0 \to n$}
                \For{$l=0 \to n$}
                \State$\Omega_{k,l}\gets K(\vb{X}_k,\vb{X}_l)$
                \EndFor
                \EndFor
                \State $\vb{H} \gets \Omega + \frac{\vb{I}}{\gamma}$
                % \State $\vb{1} \gets [1,1,...,1]$
                \State $\vb{A} \gets [[]]$ \Comment{Construct left hand side matrix}
                \State $\vb{A}_{0,0} \gets 0$
                \For{$k=0 \to n$}
                \State $\vb{A}_{k+1,0} \gets 1$
                \State $\vb{A}_{0,k+1} \gets 1$
                \EndFor
                \For{$k=0 \to n$}
                \For{$l=0 \to n$}
                \State $\vb{A}_{k+1,l+1} \gets \vb{H}_{k,l}$
                \EndFor
                \EndFor
                \State $\vb{B}\gets []$ \Comment{Construct left hand side of the equation}
                \State $\vb{B}_0 \gets 0$
                \For{$k=0 \to n$}
                \State $\vb{B}_{k+1} \gets \vb{y}_{k}$
                \EndFor
                \State $\vb{A}^{\dagger} \gets pseudoInverse(\vb{A})$ \Comment{Compute solution using pseudo inverse}
                \State $\vb{S} \gets \vb{A}^{\dagger}\vb{B}$
                \State $b \gets \vb{S}_{0}$
                \For{$k=0 \to n$}
                \State $\vb{\alpha}_{k} \gets \vb{S}_{k+1}$
                \EndFor
        \end{algorithmic}
\end{algorithm}

The basic psuedocode from the LSSVM Equation for function regression is defined in \cref{alg1:cap} for a training set of length $n$, samples $\vb{X}$, and sample labels $\vb{y}$. Training the LSSVM means computing the values of langrange multipliers $\vb{\alpha}$ and bias $b$. $K$ is the kernel function used to compute the inner products in high dimensional space, here we asume the RBF kernel. $\vb{A}$ is a matrix of size $n+1$ by $n+1$. $\vb{H}$ is a matrix of size $n$ by $n$. $\vb{I}$ is the identity. $\vb{B}$ is a vector of size $n+1$. $\vb{S}$ is a vector of size $n+1$.

The use of the Moore Penrose inverse or commonly known as psuedoinverse is such that cases where the inverse is not computable, the psuedoinverse is still guaranteed to exist. Note that matrix $A$ is square, so the use of psuedoinverse is for cases such as singular matrices.

\section{Prediction}
After training the model can be used for prediction of unseen samples. The psuedocode for prediction is shown in \cref{alg2:cap} for prediction samples $\vb{U}$. The trained model uses the learned multipliers of training samples $\vb{\alpha}$, the training samples $\vb{X}$, and the bias $b$.

\begin{algorithm}[H]
        \caption{LSSVR Prediction}\label{alg2:cap}
        \begin{algorithmic}[1]
                \Require $\vb{U}, \vb{\alpha}, \vb{X}, b$
                \Ensure $\vb{v}$
                \State $\Omega \gets [[]]$ \Comment{Construct matrix of inner products in high dimensional space}

                \For{$k=0 \to m$}
                \For{$l=0 \to n$}
                \State$\Omega_{k,l}\gets K(\vb{U}_{k},\vb{X}_l)$
                \EndFor
                \EndFor
                \State $\vb{v} \gets \Omega\vb{\alpha} + \vb{1}_{m}b$
        \end{algorithmic}
\end{algorithm}

\chapter{Example Calculation}
\section{Training}
In this exmple we will be using the function $2x^2+4$. The values of this function can be seen in Table \ref{table:1}.

\begin{table}[H]
        \centering
        \begin{tabular}{@{}lll@{}}
                \toprule
                No & x       & y       \\ \midrule
                1  & 0.0     & 4.0     \\
                2  & 0.33... & 4.22... \\
                3  & 0.66... & 4.88... \\
                4  & 1.0     & 6.0     \\
                \bottomrule
        \end{tabular}
        \caption{Example data of function $2x^2+4$} \label{table:1}
\end{table}
For $\Omega_{i,j}=K(x_i,x_j)=exp \left(-\frac{\norm*{x_i-x_j}^2}{2\sigma^2}\right)$

For example, with $\sigma=1$, $x_i=0.0$, \& $x_j = 0.33\dots$
\begin{equation}
        \begin{split}
                K(0.0,0.33) & = exp\left(-\frac{\norm*{0.0-0.33}^2}{2(1)^2}\right) \\
                & = exp\left(-\frac{0.33^2}{2}\right) \\
                & = 0.9460
        \end{split}
\end{equation}

\begin{equation}
        \Omega\gets
        \begin{bmatrix}
                1.0000 & 0.9460 & 0.8007 & 0.6065 \\
                0.9460 & 1.0000 & 0.9460 & 0.8007 \\
                0.8007 & 0.9460 & 1.0000 & 0.9460 \\
                0.6065 & 0.8007 & 0.9460 & 1.0000 \\
        \end{bmatrix}
\end{equation}

\begin{equation}
        \vb{I}\frac{1}{\gamma}\to\vb{I}\frac{1}{5}\to
        \begin{bmatrix}
                0.2000 & 0.0000 & 0.0000 & 0.0000 \\
                0.0000 & 0.2000 & 0.0000 & 0.0000 \\
                0.0000 & 0.0000 & 0.2000 & 0.0000 \\
                0.0000 & 0.0000 & 0.0000 & 0.2000
        \end{bmatrix}
\end{equation}

\begin{equation}
        \Omega+\vb{I}\frac{1}{5}\to H\to \begin{bmatrix}
                1.2000 & 0.9460 & 0.8007 & 0.6065 \\
                0.9460 & 1.2000 & 0.9460 & 0.8007 \\
                0.8007 & 0.9460 & 1.2000 & 0.9460 \\
                0.6065 & 0.8007 & 0.9460 & 1.2000 \\
        \end{bmatrix}
\end{equation}

\begin{equation}
        A\to \begin{bmatrix}
                0.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 \\
                1.0000 & 1.2000 & 0.9460 & 0.8007 & 0.6065 \\
                1.0000 & 0.9460 & 1.2000 & 0.9460 & 0.8007 \\
                1.0000 & 0.8007 & 0.9460 & 1.2000 & 0.9460 \\
                1.0000 & 0.6065 & 0.8007 & 0.9460 & 1.2000 \\
        \end{bmatrix}
\end{equation}

\begin{equation}
        B\to \begin{bmatrix}
                0.0000 \\
                4.0000 \\
                4.2222 \\
                4.8889 \\
                6.0000 \\
        \end{bmatrix}
\end{equation}

\begin{equation}
        A^{\dag}\to \begin{bmatrix}
                -0.8994 & 0.4348  & 0.0652  & 0.0652  & 0.4348  \\
                0.4348  & 2.0686  & -1.6490 & -0.5292 & 0.1096  \\
                0.0652  & -1.6490 & 3.3774  & -1.1992 & -0.5292 \\
                0.0652  & -0.5292 & -1.1992 & 3.3774  & -1.6490 \\
                0.4348  & 0.1096  & -0.5292 & -1.6490 & 2.0686  \\
        \end{bmatrix}
\end{equation}

\begin{equation}
        A^{\dag}B\to S\to \begin{bmatrix}
                4.9421  \\
                -0.6177 \\
                -1.3737 \\
                -0.5625 \\
                2.5538  \\
        \end{bmatrix}
\end{equation}

From the first element of $S$ we get the bias
\begin{equation}
        b\to 4.9421
\end{equation}

The rest of the elements are our learned multipliers
\begin{equation}
        \alpha \to \begin{bmatrix}
                -0.6177 \\
                -1.3737 \\
                -0.5625 \\
                2.5538  \\
        \end{bmatrix}
\end{equation}

\section{Prediction}
We are predicting $v$ for $U$
\begin{equation}
        U\to \begin{bmatrix}
                0.3 \\
                0.2 \\
                0.5
        \end{bmatrix}
\end{equation}

Computing $\Omega$ from the kernel we used in training and the training data we get
\begin{equation}
        \Omega\to \begin{bmatrix}
                0.9560 & 0.9994 & 0.9350 & 0.7827 \\
                0.9802 & 0.9912 & 0.8968 & 0.7261 \\
                0.8825 & 0.9862 & 0.9862 & 0.8825 \\
        \end{bmatrix}
\end{equation}

Matrix multiplication of $\Omega$ with the multipliers we solved for in training we get
\begin{equation}
        \Omega\alpha\to \begin{bmatrix}
                -0.4904 \\
                -0.6170 \\
                -0.2008 \\
        \end{bmatrix}
\end{equation}

Finaly the after adding the bias to each input we get
\begin{equation}
        \Omega\alpha+b\vb{1}_{m}\to v\to \begin{bmatrix}
                4.4516 \\
                4.3251 \\
                4.7413 \\
        \end{bmatrix}
\end{equation}
Where $\vb{1}_{m}$ is a vector of 1s with the length of $U$.


\end{document}